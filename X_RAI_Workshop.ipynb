{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandosanti06/Ballot-Scanner/blob/main/X_RAI_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X-RAI\n",
        "\n",
        "This project uses deep learning to automatically detect bone fractures from musculoskeletal X-ray images using the MURA v1.1 dataset. A lightweight convolutional neural network (MobileNetV2) is trained to classify each image as “Fractured” or “Not Fractured.”\n",
        "\n",
        "\n",
        "Stack Used for the project:\n",
        "\n",
        "- Dataset: MURA v1.1 (Stanford ML Group)\n",
        "- Model: MobileNetV2 (pretrained on ImageNet)\n",
        "- Frameworks: PyTorch, Torchvision\n",
        "- Visualization: Grad-CAM, Matplotlib\n",
        "- Environment: Google Colab / Kaggle (GPU)\n",
        "\n",
        "Features:\n",
        "- Fracture classification with up to ~86% accuracy on validation subset\n",
        "- Fast training setup using MobileNet + resized images\n",
        "- Grad-CAM visualizations to interpret model decisions\n",
        "- Label and prediction overlays with probabilities\n"
      ],
      "metadata": {
        "id": "4i8CoFzd9EHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install & Download MURA v.1.1 Dataset\n",
        "\n",
        "**Description**: This code block installs the kagglehub package, imports it, and then uses it to download the MURA v1.1 dataset from Kaggle. The downloaded dataset is stored in a directory, and the path to this directory is printed."
      ],
      "metadata": {
        "id": "7SGjAKJOjc1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download MURA dataset using kagglehub\n",
        "path = kagglehub.dataset_download(\"cjinny/mura-v11\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z5xTNqa9NQF",
        "outputId": "e7cd119b-1142-4701-e003-be0588280c47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/mura-v11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select the part of the body\n",
        "\n",
        "**Description**: This code block selects the specific body part to be analyzed (e.g., \"humerus\", \"wrist\"). Based on the selected body part, it sets the paths to the corresponding training, validation, and test image folders within the MURA dataset."
      ],
      "metadata": {
        "id": "4BLC0Ua49Ta2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bodypart = \"humerus\" #change for different parts of the body.\n",
        "if bodypart == \"wrist\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_WRIST\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_WRIST\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_WRIST\"\n",
        "elif bodypart == \"elbow\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_ELBOW\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_ELBOW\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_ELBOW\"\n",
        "elif bodypart == \"shoulder\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_SHOULDER\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_SHOULDER\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_SHOULDER\"\n",
        "elif bodypart == \"humerus\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_HUMERUS\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_HUMERUS\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_HUMERUS\"\n",
        "elif bodypart == \"forearm\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_FOREARM\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_FOREARM\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_FOREARM\"\n",
        "elif bodypart == \"hand\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_HAND\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_HAND\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_HAND\"\n",
        "elif bodypart == \"finger\":\n",
        "    data_dir = f\"{path}/MURA-v1.1/train/XR_FINGER\"\n",
        "    valid_dir = f\"{path}/MURA-v1.1/valid/XR_FINGER\"\n",
        "    test_dir = f\"{path}/MURA-v1.1/test/XR_FINGER\"\n",
        "else:\n",
        "    print(\"Invalid bodypart\")"
      ],
      "metadata": {
        "id": "G08Zzzey9aSL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Libraries\n",
        "\n",
        "**Description**: This code block installs the necessary Python libraries for deep learning (PyTorch, torchvision), image processing (Pillow), visualization (matplotlib), and progress tracking (tqdm). It then imports the required modules from these libraries."
      ],
      "metadata": {
        "id": "wGjI9q_m9nKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision matplotlib scikit-learn tqdm grad-cam\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from torchvision.models import mobilenet_v2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\n",
        "import random\n",
        "import time\n",
        "print(\"Done importing libraires\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMW4PxRk9ryh",
        "outputId": "f8849a9a-fbbe-4a2e-8018-4a11084085ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/7.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m5.7/7.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Setup\n",
        "\n",
        "**Description**: This code block defines a custom PyTorch Dataset class called MURADataset. This class is responsible for loading and preprocessing the MURA dataset images and labels. It handles tasks like finding image files, assigning labels (0 for normal, 1 for fractured), and applying transformations."
      ],
      "metadata": {
        "id": "M2pvmT5i9sjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MURADataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        for root, _, files in os.walk(root_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')) and not file.startswith('._'):\n",
        "                    path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        Image.open(path).verify()\n",
        "                        label = 1 if 'positive' in root.lower() else 0\n",
        "                        self.image_paths.append(path)\n",
        "                        self.labels.append(label)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(self.labels[idx], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "jpz5h82c9wEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms\n",
        "\n",
        "- Defines preprocessing steps for each image:\n",
        "- Resize to 424x424 pixels.\n",
        "- Convert image to a PyTorch tensor.\n",
        "  - Imagine a picture made of tiny blocks called pixels—each pixel has colors like red, green, and blue.\n",
        "  \n",
        "    Now, pretend you’re turning that picture into LEGO instructions so a robot can understand and rebuild it. Each block (pixel) gets turned into numbers that say how much red, green, and blue it has. All those numbers go into a big box called a tensor—like a super smart list of numbers.\n",
        "\n",
        "So, converting an image to a tensor means turning a picture into numbers a computer can understand and work with!\n",
        "- Normalize pixel values to range [-1, 1] using mean and std of 0.5 for all 3 RGB channels.\n",
        "  - Imagine you have a coloring book and some crayons. Some pages are really bright, and some are kind of dull. To make them all look more the same, you might color the bright ones a little lighter and the dull ones a little brighter.\n",
        "    \n",
        "    That’s kind of like what we do when we normalize a picture. We look at how bright or dark the colors usually are (that’s the mean) and how different they are from that (that’s the standard deviation), and then we fix them so they all look more balanced—not too bright, not too dark.\n"
      ],
      "metadata": {
        "id": "KIX_CAxY91qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((424, 424)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])"
      ],
      "metadata": {
        "id": "uhpDG1ze96L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset and limit size\n",
        "\n",
        "**Description**: This code block loads the MURA dataset using the custom MURADataset class and applies the defined transformations. It also limits the dataset size for faster training and testing during development by selecting a subset of samples.\n"
      ],
      "metadata": {
        "id": "f65lkIuy99qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = MURADataset(data_dir, transform)\n",
        "print(\"Full dataset:\", len(full_dataset))\n",
        "\n",
        "# Edit the commented code below to limit the training data to speed up the learning process.\n",
        "\n",
        "#subset_indices = list(range(min(1000, len(full_dataset))))\n",
        "#full_dataset = Subset(full_dataset, subset_indices)"
      ],
      "metadata": {
        "id": "8TJpzvLh99Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train valid split\n",
        "\n",
        "**Description**: This code block splits the dataset into training and validation sets. It calculates the lengths of each set based on a specified ratio (e.g., 80% for training, 20% for validation) and uses random_split to create the subsets. It then creates data loaders for both sets.\n"
      ],
      "metadata": {
        "id": "ontI3k43-E2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the training method from 50:50 to 80:20\n",
        "train_len = int(0.5 * len(full_dataset))\n",
        "val_len = len(full_dataset) - train_len\n",
        "train_set, val_set = random_split(full_dataset, [train_len, val_len])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=8)"
      ],
      "metadata": {
        "id": "Wjib5md4-Fwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet Model\n",
        "\n",
        "**Description**: This code block initializes a MobileNetV2 model, which is a pre-trained convolutional neural network. It modifies the model's classifier to output a single value (for binary classification) and moves it to the appropriate device (GPU if available, otherwise CPU). It also defines the loss function and optimizer for training.\n"
      ],
      "metadata": {
        "id": "stUZ6XM3-IjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 1)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "XWCObkhI-NPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "**Description**: This code block contains the training loop for the model. It iterates over the training data for a specified number of epochs, calculates the loss, updates the model's weights using the optimizer, and tracks the training progress."
      ],
      "metadata": {
        "id": "9fl0XdgA-QKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best number of epochs for the best performance.\n",
        "num_epochs = 20\n",
        "epoch_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start = time.time()\n",
        "    train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "\n",
        "    for images, labels in train_iter:\n",
        "        images, labels = images.to(device), labels.unsqueeze(1).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        train_iter.set_postfix(loss=loss.item())\n",
        "    epoch_loss.append(total_loss/len(train_loader))\n",
        "    if epoch+1 != num_epochs:\n",
        "      print(f\"✅ Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Time: {time.time()-start:.1f}s\")\n",
        "      print(\"\")\n",
        "    elif epoch+1 == num_epochs - 1:\n",
        "      print(f\"✅ Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Time: {time.time()-start:.1f}s\")\n",
        "    else:\n",
        "      print(\"\")\n",
        "      print(f\"✅ Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Time: {time.time()-start:.1f}s\")\n",
        "print(\"\")\n",
        "print(f\"Epoch with the lowest loss is: Epoch {epoch_loss.index(min(epoch_loss))+1} | Loss: {round(min(epoch_loss),4)}\")"
      ],
      "metadata": {
        "id": "qkU-WPbG-TQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalution\n",
        "\n",
        "**Description**: This code block evaluates the trained model on the validation set. It makes predictions, calculates evaluation metrics (like precision, recall, and F1-score), and prints a classification report summarizing the model's performance."
      ],
      "metadata": {
        "id": "_ZueyvR2-YpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        outputs = model(images.to(device))\n",
        "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"Not Fractured\", \"Fractured\"]))\n"
      ],
      "metadata": {
        "id": "ltpjI1Qf-Z_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC Curve and Per-Class Accuracy Tests\n",
        "**Description**: This code block computes and visualizes the ROC (Receiver Operating Characteristic) curve to evaluate the trade-off between true positive and false positive rates for the binary classification task. It calculates the Area Under the Curve (AUC) to quantify the model’s overall ability to distinguish between “Fractured” and “Not Fractured” classes. Additionally, it computes per-class accuracy by isolating predictions for each class and measuring how accurately the model performs on each individually. The ROC curve is plotted with labeled axes, a diagonal reference line, and a legend showing the AUC score."
      ],
      "metadata": {
        "id": "iyzbxZmKd1Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ROC Curve & Per-Class Accuracy ---\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Per-Class Accuracy\n",
        "labels = [\"Not Fractured\", \"Fractured\"]\n",
        "for i, label in enumerate(labels):\n",
        "    idx = (all_labels == i)\n",
        "    acc = accuracy_score([all_labels[idx]], [all_preds[idx]])\n",
        "    # print(f\"{label} Accuracy: {acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "_KGS0cP1d07D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-CAM\n",
        "\n",
        "**Description**: This code block uses Grad-CAM (Gradient-weighted Class Activation Mapping) to visualize the areas of the input image that are most important for the model's prediction. It selects a random image from the validation set, performs a forward pass, and then uses Grad-CAM to generate a heatmap highlighting the relevant regions. Finally, it displays the original image and the heatmap side-by-side with labels and predictions."
      ],
      "metadata": {
        "id": "MrSctmnt-gZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a random sample from the validation set\n",
        "random_idx = random.randint(0, len(val_set) - 1)\n",
        "image_tensor, label = val_set[random_idx]\n",
        "# Forward pass for prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "    output = model(input_tensor)\n",
        "    pred_prob = torch.sigmoid(output).item()\n",
        "    pred_class = int(pred_prob > 0.5)\n",
        "\n",
        "# Label formatting\n",
        "# Add a not sure prediction when the probability is close to 50%.\n",
        "label_text = \"Fractured\" if int(label.item()) == 1 else \"Not Fractured\"\n",
        "if pred_class == 1:\n",
        "    pred_text = \"Fractured\"\n",
        "else:\n",
        "    pred_text = \"Not Fractured\"\n",
        "\n",
        "# Grad-CAM setup\n",
        "target_layer = model.features[-1]\n",
        "cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=[BinaryClassifierOutputTarget(1)])[0]\n",
        "\n",
        "# Prepare images\n",
        "img_np = image_tensor.permute(1, 2, 0).numpy()\n",
        "img_np = (img_np * 0.5) + 0.5\n",
        "gradcam_img = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "# Side-by-side with labels\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_np)\n",
        "plt.title(f\"Original | Actual: {label_text}\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gradcam_img)\n",
        "if pred_class == 1:\n",
        "    plt.title(f\"Grad-CAM | Prediction: {pred_text} ({pred_prob * 100:.2f}%)\")\n",
        "else:\n",
        "  plt.title(f\"Grad-CAM | Prediction: {pred_text} ({100 - pred_prob:.2f}%)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P7bAYacd-hk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}